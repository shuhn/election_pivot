{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from name_dict import parse_names, file_grab\n",
    "import re\n",
    "import numpy as np\n",
    "import itertools\n",
    "import string\n",
    "import operator\n",
    "from textblob import TextBlob\n",
    "from summa import summarizer\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict1 = {\"H\": [\"I said this today.\"], \"D\": [\"I am TRUMP.\"]}\n",
    "dict2 = {\"H\": [\"I said this yesterday.\"], \"D\": [\"I am DRUMF.\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "relevant_debates = [dict1, dict2]\n",
    "aggregate_dict = {}\n",
    "for d in relevant_debates:\n",
    "    for k, v in d.iteritems():\n",
    "        if aggregate_dict.has_key(k):\n",
    "            aggregate_dict[k] = aggregate_dict[k] + v\n",
    "        else:\n",
    "            aggregate_dict[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D': ['I am TRUMP.', 'I am DRUMF.'],\n",
       " 'H': ['I said this today.', 'I said this yesterday.']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blob = TextBlob('Hi my name is Scotty. I am DS.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi my name is Scotty.\n",
      "I am DS.\n"
     ]
    }
   ],
   "source": [
    "for sentence in blob.sentences:\n",
    "    print sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def setup_agg_df(relevant_debates, defaults = ['H', 'D']):\n",
    "    #aggregate dictionaries\n",
    "    aggregate_dict = {}\n",
    "    for d in relevant_debates:\n",
    "        for k, v in d.iteritems():\n",
    "            if aggregate_dict.has_key(k):\n",
    "                aggregate_dict[k] = aggregate_dict[k] + v\n",
    "            else:\n",
    "                aggregate_dict[k] = v\n",
    "\n",
    "    #setup df - column = candidate name, rows = sentences\n",
    "    df_list = []\n",
    "    column_list = []\n",
    "    flag = False\n",
    "    for candidate in defaults:\n",
    "        sentence_list = []\n",
    "        list_words = aggregate_dict[candidate]\n",
    "        string_words = (' '.join(list_words))\n",
    "        textblob = TextBlob(string_words)\n",
    "        for sentence in textblob.sentences:\n",
    "            sentence_list.append(str(sentence))\n",
    "        if flag == False: \n",
    "            df = pd.DataFrame(sentence_list, columns = [candidate])\n",
    "            flag = True\n",
    "        else: \n",
    "            df[candidate] = sentence_list\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I said this today.</td>\n",
       "      <td>I am TRUMP.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I said this yesterday.</td>\n",
       "      <td>I am DRUMF.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        H            D\n",
       "0      I said this today.  I am TRUMP.\n",
       "1  I said this yesterday.  I am DRUMF."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_modeling(relevant_debates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordListCorpusReader in u'/Users/scotthuhn/nltk_data/corpora/stopwords'>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    regex = re.compile('<.+?>|[^a-zA-Z]')\n",
    "    clean_txt = regex.sub(' ', text)\n",
    "    tokens = clean_txt.split()\n",
    "    lowercased = [t.lower() for t in tokens]\n",
    "    no_stopwords = [w for w in lowercased if not w in stopwords.words('english')]\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmed = [wordnet_lemmatizer.lemmatize(w) for w in no_stopwords]\n",
    "    return [w for w in lemmed if w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line = \"this is a line of text starting here.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'line text starting'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tokenize(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"pos\"}\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "data = urllib.urlencode({\"text\": \"I'm a very good boy \"}) \n",
    "u = urllib.urlopen(\"http://text-processing.com/api/sentiment/\", data)\n",
    "the_page = u.read()\n",
    "print the_page.split(':')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list1 = ['unidentified', 'male', 'applause', 'laughter', 'well', 'know', 'let', 'crosstalk', 'thanks', 'thank', 'you', 'cross', 'talk', 'booing', 'good', 'lot', 'point', 'going','say', 'want', 'year', 'inaudible', 'know', 'think', 'later']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unidentified', 'male', 'applause', 'laughter', 'well', 'know', 'let', 'crosstalk', 'thanks', 'thank', 'you', 'cross', 'talk', 'booing', 'good', 'lot', 'point', 'going', 'say', 'want', 'year', 'inaudible', 'know', 'think', 'later']\n"
     ]
    }
   ],
   "source": [
    "print list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
